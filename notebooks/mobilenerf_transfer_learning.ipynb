# Transfer Learning for Scene Recreation using Keras
# =====================================================

# Import necessary libraries
import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from glob import glob

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# 1. Define configuration parameters
# ===============================

INPUT_SIZE = (224, 224)
BATCH_SIZE = 16
EPOCHS = 50
LEARNING_RATE = 1e-4
DATA_DIR = 'data/processed'
OUTPUT_DIR = 'outputs'
MODEL_CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, 'model_checkpoint.h5')
FINAL_MODEL_PATH = os.path.join(OUTPUT_DIR, 'final_model.h5')
TENSORBOARD_LOG_DIR = os.path.join(OUTPUT_DIR, 'logs')

# Create output directory if it doesn't exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(TENSORBOARD_LOG_DIR, exist_ok=True)

# 2. Load and preprocess data
# ==============================

def get_dataset_files(data_dir, split='train'):
    """Get image files for the specified split."""
    split_file = os.path.join(data_dir, f'{split}_files.txt')
    
    if os.path.exists(split_file):
        with open(split_file, 'r') as f:
            files = [os.path.join(data_dir, line.strip()) for line in f.readlines()]
    else:
        # If split file doesn't exist, create one
        all_files = glob(os.path.join(data_dir, '*.jpg')) + glob(os.path.join(data_dir, '*.png'))
        np.random.shuffle(all_files)
        
        # Split files (80% train, 10% val, 10% test)
        n_total = len(all_files)
        n_train = int(0.8 * n_total)
        n_val = int(0.1 * n_total)
        
        if split == 'train':
            files = all_files[:n_train]
        elif split == 'val':
            files = all_files[n_train:n_train+n_val]
        else:  # test
            files = all_files[n_train+n_val:]
            
        # Save the split
        with open(split_file, 'w') as f:
            for file in files:
                f.write(f"{os.path.basename(file)}\n")
    
    return files

# Load image paths
train_files = get_dataset_files(DATA_DIR, 'train')
val_files = get_dataset_files(DATA_DIR, 'val')

print(f"Training files: {len(train_files)}")
print(f"Validation files: {len(val_files)}")

# Display a few sample images
plt.figure(figsize=(15, 10))
for i in range(min(5, len(train_files))):
    img = cv2.imread(train_files[i])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(1, 5, i+1)
    plt.imshow(img)
    plt.title(f"Sample {i+1}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# Data preprocessing function
def preprocess_image(image_path, target_size=INPUT_SIZE):
    """Load and preprocess an image."""
    # Read image
    img = tf.io.read_file(image_path)
    img = tf.image.decode_image(img, channels=3, expand_animations=False)
    
    # Resize
    img = tf.image.resize(img, target_size)
    
    # Normalize to [0, 1]
    img = tf.cast(img, tf.float32) / 255.0
    
    return img

# Create TensorFlow datasets
def create_dataset(file_paths, batch_size=BATCH_SIZE, is_training=True):
    """Create a TensorFlow dataset from file paths."""
    ds = tf.data.Dataset.from_tensor_slices(file_paths)
    
    # Map preprocessing function
    ds = ds.map(
        lambda x: (preprocess_image(x), preprocess_image(x)),  # Input and target are the same for autoencoder
        num_parallel_calls=tf.data.AUTOTUNE
    )
    
    # Batch and prefetch
    if is_training:
        ds = ds.shuffle(buffer_size=len(file_paths))
        ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    else:
        ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    return ds

# Create datasets
train_dataset = create_dataset(train_files, is_training=True)
val_dataset = create_dataset(val_files, is_training=False)

# 3. Build the MobileNeRF model architecture
# =========================================

# Encoder architecture (based on MobileNetV2)
def build_encoder(input_shape=(224, 224, 3), latent_dim=64):
    """Build the encoder using MobileNetV2 as base."""
    # Use MobileNetV2 as base model for transfer learning
    base_model = MobileNetV2(
        input_shape=input_shape,
        include_top=False,
        weights='imagenet'
    )
    
    # Freeze the base model initially
    base_model.trainable = False
    
    # Build encoder
    inputs = keras.Input(shape=input_shape)
    x = base_model(inputs)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(latent_dim * 4, activation='relu')(x)
    latent = layers.Dense(latent_dim, name='latent_vector')(x)
    
    # Reshape for decoder input
    decoder_shape = (7, 7, latent_dim)  # Adjust based on your requirements
    x = layers.Dense(np.prod(decoder_shape), activation='relu')(latent)
    x = layers.Reshape(decoder_shape)(x)
    
    encoder = Model(inputs, x, name='encoder')
    return encoder

# Decoder architecture
def build_decoder(input_shape=(7, 7, 64)):
    """Build the decoder for scene recreation."""
    inputs = keras.Input(shape=input_shape)
    
    # Upsampling blocks
    x = layers.Conv2D(256, 3, padding='same', activation='relu')(inputs)
    x = layers.UpSampling2D(2)(x)  # 14x14
    
    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)
    x = layers.UpSampling2D(2)(x)  # 28x28
    
    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = layers.UpSampling2D(2)(x)  # 56x56
    
    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)
    x = layers.UpSampling2D(2)(x)  # 112x112
    
    x = layers.Conv2D(16, 3, padding='same', activation='relu')(x)
    x = layers.UpSampling2D(2)(x)  # 224x224
    
    # Output layer
    outputs = layers.Conv2D(3, 3, padding='same', activation='sigmoid')(x)
    
    decoder = Model(inputs, outputs, name='decoder')
    return decoder

# Full model
def build_model(input_shape=(224, 224, 3), latent_dim=64):
    """Build the full MobileNeRF model."""
    # Build encoder and decoder
    encoder = build_encoder(input_shape, latent_dim)
    decoder = build_decoder((7, 7, latent_dim))
    
    # Create full model
    inputs = keras.Input(shape=input_shape)
    encoded = encoder(inputs)
    outputs = decoder(encoded)
    
    model = Model(inputs, outputs, name='mobilenerf')
    return model, encoder, decoder

# Build the model
model, encoder, decoder = build_model(input_shape=(*INPUT_SIZE, 3), latent_dim=64)

# Display model summary
model.summary()

# 4. Define training callbacks
# ========================

callbacks = [
    ModelCheckpoint(
        MODEL_CHECKPOINT_PATH,
        monitor='val_loss',
        save_best_only=True,
        mode='min',
        verbose=1
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    ),
    tf.keras.callbacks.TensorBoard(
        log_dir=TENSORBOARD_LOG_DIR,
        histogram_freq=1
    )
]

# 5. Compile and train the model
# ==========================

# Compile the model
model.compile(
    optimizer=Adam(LEARNING_RATE),
    loss='mse'
)

# Phase 1: Train with frozen encoder
print("Phase 1: Training with frozen encoder...")
history1 = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS // 2,  # Train for half the epochs
    callbacks=callbacks
)

# Phase 2: Fine-tune with unfrozen encoder
print("Phase 2: Fine-tuning with unfrozen encoder...")
# Unfreeze the encoder
for layer in encoder.layers:
    if isinstance(layer, keras.models.Model):  # This is the MobileNetV2 base model
        # Unfreeze the top layers of the base model
        for i, base_layer in enumerate(layer.layers):
            if i > len(layer.layers) - 10:  # Unfreeze the last 10 layers
                base_layer.trainable = True

# Recompile with lower learning rate
model.compile(
    optimizer=Adam(LEARNING_RATE / 10),
    loss='mse'
)

history2 = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS // 2,  # Train for the remaining epochs
    callbacks=callbacks,
    initial_epoch=history1.epoch[-1] + 1 if history1.epoch else 0
)

# Save the final model
model.save(FINAL_MODEL_PATH)
print(f"Model saved to {FINAL_MODEL_PATH}")

# 6. Visualize training history
# =========================

# Combine histories
combined_history = {
    'loss': history1.history['loss'] + history2.history['loss'],
    'val_loss': history1.history['val_loss'] + history2.history['val_loss']
}

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(combined_history['loss'])
plt.plot(combined_history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

# 7. Test the model on some examples
# ==============================

# Get test files
test_files = get_dataset_files(DATA_DIR, 'test')

# Select a few test images
test_indices = np.random.choice(len(test_files), min(5, len(test_files)), replace=False)
test_images = [test_files[i] for i in test_indices]

# Visualize original and reconstructed images
plt.figure(figsize=(15, 10))
for i, image_path in enumerate(test_images):
    # Load and preprocess image
    img = preprocess_image(image_path).numpy()
    
    # Predict
    reconstructed = model.predict(np.expand_dims(img, axis=0))[0]
    
    # Plot original
    plt.subplot(2, len(test_images), i + 1)
    plt.imshow(img)
    plt.title(f"Original {i+1}")
    plt.axis('off')
    
    # Plot reconstruction
    plt.subplot(2, len(test_images), len(test_images) + i + 1)
    plt.imshow(reconstructed)
    plt.title(f"Reconstructed {i+1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# 8. Export model for edge deployment
# ================================

# Function to convert model to TensorFlow Lite
def convert_to_tflite(model, output_path):
    """Convert a Keras model to TensorFlow Lite format."""
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # Enable optimizations
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    # Convert the model
    tflite_model = converter.convert()
    
    # Save the model
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    print(f"Model converted to TFLite and saved to {output_path}")
    return output_path

# Function to quantize the model for edge deployment
def quantize_model(model, output_path):
    """Quantize model to int8 for edge deployment."""
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # Enable optimizations and quantization
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.representative_dataset = representative_dataset_gen
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    
    # Convert the model
    tflite_model = converter.convert()
    
    # Save the model
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    print(f"Model quantized and saved to {output_path}")
    return output_path

# Create a representative dataset generator
def representative_dataset_gen():
    """Generator function to provide representative dataset for quantization."""
    for data in train_dataset.take(100):  # Use 100 samples for calibration
        input_data = data[0].numpy()
        for i in range(len(input_data)):
            yield [np.expand_dims(input_data[i], axis=0)]

# Export models
tflite_path = os.path.join(OUTPUT_DIR, 'mobilenerf.tflite')
quantized_path = os.path.join(OUTPUT_DIR, 'mobilenerf_quantized.tflite')

# Convert to TFLite
convert_to_tflite(model, tflite_path)

# Quantize model
try:
    quantize_model(model, quantized_path)
except Exception as e:
    print(f"Quantization failed: {e}")
    print("Quantization requires a full representative dataset. Skipping for now.")

# 9. Helper function to benchmark the TFLite model
# ============================================

def benchmark_tflite_model(tflite_model_path, num_runs=50):
    """Benchmark TFLite model performance."""
    # Load the TFLite model
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
    interpreter.allocate_tensors()
    
    # Get input and output tensors
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    # Create a dummy input
    input_shape = input_details[0]['shape']
    input_data = np.random.random(input_shape).astype(np.float32)
    
    # Warm-up run
    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    
    # Benchmark
    import time
    start_time = time.time()
    
    for _ in range(num_runs):
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
    
    end_time = time.time()
    
    # Calculate metrics
    total_time = end_time - start_time
    avg_time = total_time / num_runs
    fps = num_runs / total_time
    
    print(f"Model: {os.path.basename(tflite_model_path)}")
    print(f"Average inference time: {avg_time*1000:.2f} ms")
    print(f"FPS: {fps:.2f}")
    
    return {
        'avg_inference_time_ms': avg_time * 1000,
        'fps': fps
    }

# Benchmark the models
print("Benchmarking standard TFLite model:")
standard_metrics = benchmark_tflite_model(tflite_path)

if os.path.exists(quantized_path):
    print("\nBenchmarking quantized TFLite model:")
    quantized_metrics = benchmark_tflite_model(quantized_path)

# Display model sizes
standard_size = os.path.getsize(tflite_path) / (1024 * 1024)  # Convert to MB
print(f"\nStandard TFLite model size: {standard_size:.2f} MB")

if os.path.exists(quantized_path):
    quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)
    print(f"Quantized TFLite model size: {quantized_size:.2f} MB")

# 10. Visualize and analyze the model's feature maps
# ==============================================

def visualize_feature_maps(model, image_path, layer_name):
    """Visualize feature maps for a specific layer of the model."""
    # Create a model that outputs the feature maps
    layer_outputs = [layer.output for layer in model.layers if layer.name == layer_name]
    if not layer_outputs:
        print(f"Layer '{layer_name}' not found in the model")
        return
    
    feature_map_model = Model(inputs=model.input, outputs=layer_outputs[0])
    
    # Load and preprocess image
    img = preprocess_image(image_path).numpy()
    
    # Get feature maps
    feature_maps = feature_map_model.predict(np.expand_dims(img, axis=0))
    
    # Plot feature maps
    fig = plt.figure(figsize=(15, 10))
    
    # Plot original image
    plt.subplot(1, 2, 1)
    plt.imshow(img)
    plt.title("Original Image")
    plt.axis('off')
    
    # Plot feature maps
    n_features = min(16, feature_maps.shape[-1])  # Display up to 16 feature maps
    grid_size = int(np.ceil(np.sqrt(n_features)))
    
    plt.subplot(1, 2, 2)
    feature_map_grid = np.zeros((grid_size*8, grid_size*8))
    
    for i in range(grid_size):
        for j in range(grid_size):
            idx = i * grid_size + j
            if idx < n_features:
                feature_map = feature_maps[0, :, :, idx]
                feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8)
                start_i, start_j = i * 8, j * 8
                feature_map_grid[start_i:start_i+8, start_j:start_j+8] = cv2.resize(feature_map, (8, 8))
    
    plt.imshow(feature_map_grid, cmap='viridis')
    plt.title(f"Feature Maps from {layer_name}")
    plt.axis('off')
    
    plt.tight_layout()
    plt.show()

# Visualize feature maps for a sample image
if test_files:
    # Choose a random test image
    sample_image = np.random.choice(test_files)
    
    # Visualize feature maps from one of the decoder layers
    decoder_layers = [layer.name for layer in decoder.layers if isinstance(layer, layers.Conv2D)]
    if decoder_layers:
        visualize_feature_maps(model, sample_image, decoder_layers[0])

print("Transfer learning pipeline completed!")
